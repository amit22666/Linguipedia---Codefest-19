{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Downloads/train hack.csv\",header=None)\n",
    "test = pd.read_csv(\"../Downloads/test hack.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n Their best since \"...And Justice For All\" b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n Bob Dylan simply amazes me. He's one of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n New York bands come, New York bands go, som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\nIt may not hook and grab you immediately but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\\n A stratospheric achievement.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>\\nThe album that passed Yeezus as my favourite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>\\n Maybe I should give the band a higher numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>\\nBlack Joe Lewis's Scandalous is a fun, raw, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>\\n Played it to death, rated it my favourite a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>\\nThe Fame Monster is perfect from start to fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>\\nEvery time, I have to remind myself that thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>\\n A great introduction to Tropicalia and not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>\\nI'd heard some of these songs in more simple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>\\nEasily the best album of the year, I found L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>\\n This album is amazing. You have to listen t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>\\n\\n            This review contains spoilers,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>\\n Gotta admit that when I first heard this al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>\\n A much more subtle and evolved Hot Chip the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>\\nI'd say that this will be the best rock albu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>\\n If I could, I would give this album a 20. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>\\nExcellent record. These guys don't get half ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>\\n This is the album Wilco should have made th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>\\n This is the best stuff in a really long tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>\\n If not too modest to say, one of the best i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>\\n I give it an 85% With the exception of a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>\\n Simply a pop gem. It'll be a long while til...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>\\n Haunting lyrics with a brilliant mix of syn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>\\n Just glorious. Absolutely love it.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>\\nBlack Sun is one of the top 5 releases since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17621</th>\n",
       "      <td>17621</td>\n",
       "      <td>\\nThe most urban album of Beyoncé's career. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17622</th>\n",
       "      <td>17622</td>\n",
       "      <td>\\nThis album is so under rated... Its a master...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17623</th>\n",
       "      <td>17623</td>\n",
       "      <td>\\nLike before, Danny Brown has chosen to sound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17624</th>\n",
       "      <td>17624</td>\n",
       "      <td>\\n I know some might think an album like this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17625</th>\n",
       "      <td>17625</td>\n",
       "      <td>\\n Not as great as his last album, but that's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17626</th>\n",
       "      <td>17626</td>\n",
       "      <td>\\n this cd actually took me a while to get int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17627</th>\n",
       "      <td>17627</td>\n",
       "      <td>\\n Metal with a brain, and a soul as well.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17628</th>\n",
       "      <td>17628</td>\n",
       "      <td>\\n One of the best discs of the year. Teddy ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17629</th>\n",
       "      <td>17629</td>\n",
       "      <td>\\n listen to this now, because once it hits th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17630</th>\n",
       "      <td>17630</td>\n",
       "      <td>\\n A phenomenal hip-hop trip. Listening to thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17631</th>\n",
       "      <td>17631</td>\n",
       "      <td>\\nHonestly, it’s been so long since I’ve heard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17632</th>\n",
       "      <td>17632</td>\n",
       "      <td>\\nIt seems like just yesterday that the four N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17633</th>\n",
       "      <td>17633</td>\n",
       "      <td>\\n This is a brilliant album wothy of ten out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17634</th>\n",
       "      <td>17634</td>\n",
       "      <td>\\n actually worse than his last album!.....has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17635</th>\n",
       "      <td>17635</td>\n",
       "      <td>\\nTheir best album to date. A masterpiece. Ugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17636</th>\n",
       "      <td>17636</td>\n",
       "      <td>\\nI've waited 15 years for this album. These g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17637</th>\n",
       "      <td>17637</td>\n",
       "      <td>\\n Beautifully dark and brooding. It has great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17638</th>\n",
       "      <td>17638</td>\n",
       "      <td>\\nLemonade é um álbum excelente, com vocais su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17639</th>\n",
       "      <td>17639</td>\n",
       "      <td>\\n A great album from a great artist.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17640</th>\n",
       "      <td>17640</td>\n",
       "      <td>\\n bloc party the best band in britain\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17641</th>\n",
       "      <td>17641</td>\n",
       "      <td>\\nIs This It? is one of the masterpieces of co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17642</th>\n",
       "      <td>17642</td>\n",
       "      <td>\\n Minimalistic like Phillip Glass, but more m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17643</th>\n",
       "      <td>17643</td>\n",
       "      <td>\\n great album, with various elements. Krauss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17644</th>\n",
       "      <td>17644</td>\n",
       "      <td>\\n Awesome CD - RT singing better than ever - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17645</th>\n",
       "      <td>17645</td>\n",
       "      <td>\\nI love it, it has been almost a year but it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17646</th>\n",
       "      <td>17646</td>\n",
       "      <td>\\n Actually, nothing about this album is overr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17647</th>\n",
       "      <td>17647</td>\n",
       "      <td>\\n Wild Mountain Nation may still be their bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17648</th>\n",
       "      <td>17648</td>\n",
       "      <td>\\nThis is undoubtedly one of the most crisp an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17649</th>\n",
       "      <td>17649</td>\n",
       "      <td>\\n It's a difficult, but absolute amazing reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17650</th>\n",
       "      <td>17650</td>\n",
       "      <td>\\n Out of this world...nothing like it\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17651 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                                                  1\n",
       "0         Id                                             Review\n",
       "1          1  \\n Their best since \"...And Justice For All\" b...\n",
       "2          2  \\n Bob Dylan simply amazes me. He's one of the...\n",
       "3          3  \\n New York bands come, New York bands go, som...\n",
       "4          4  \\nIt may not hook and grab you immediately but...\n",
       "5          5                  \\n A stratospheric achievement.\\n\n",
       "6          6  \\nThe album that passed Yeezus as my favourite...\n",
       "7          7  \\n Maybe I should give the band a higher numbe...\n",
       "8          8  \\nBlack Joe Lewis's Scandalous is a fun, raw, ...\n",
       "9          9  \\n Played it to death, rated it my favourite a...\n",
       "10        10  \\nThe Fame Monster is perfect from start to fi...\n",
       "11        11  \\nEvery time, I have to remind myself that thi...\n",
       "12        12  \\n A great introduction to Tropicalia and not ...\n",
       "13        13  \\nI'd heard some of these songs in more simple...\n",
       "14        14  \\nEasily the best album of the year, I found L...\n",
       "15        15  \\n This album is amazing. You have to listen t...\n",
       "16        16  \\n\\n            This review contains spoilers,...\n",
       "17        17  \\n Gotta admit that when I first heard this al...\n",
       "18        18  \\n A much more subtle and evolved Hot Chip the...\n",
       "19        19  \\nI'd say that this will be the best rock albu...\n",
       "20        20  \\n If I could, I would give this album a 20. T...\n",
       "21        21  \\nExcellent record. These guys don't get half ...\n",
       "22        22  \\n This is the album Wilco should have made th...\n",
       "23        23  \\n This is the best stuff in a really long tim...\n",
       "24        24  \\n If not too modest to say, one of the best i...\n",
       "25        25  \\n I give it an 85% With the exception of a fe...\n",
       "26        26  \\n Simply a pop gem. It'll be a long while til...\n",
       "27        27  \\n Haunting lyrics with a brilliant mix of syn...\n",
       "28        28            \\n Just glorious. Absolutely love it.\\n\n",
       "29        29  \\nBlack Sun is one of the top 5 releases since...\n",
       "...      ...                                                ...\n",
       "17621  17621  \\nThe most urban album of Beyoncé's career. Th...\n",
       "17622  17622  \\nThis album is so under rated... Its a master...\n",
       "17623  17623  \\nLike before, Danny Brown has chosen to sound...\n",
       "17624  17624  \\n I know some might think an album like this ...\n",
       "17625  17625  \\n Not as great as his last album, but that's ...\n",
       "17626  17626  \\n this cd actually took me a while to get int...\n",
       "17627  17627       \\n Metal with a brain, and a soul as well.\\n\n",
       "17628  17628  \\n One of the best discs of the year. Teddy ha...\n",
       "17629  17629  \\n listen to this now, because once it hits th...\n",
       "17630  17630  \\n A phenomenal hip-hop trip. Listening to thi...\n",
       "17631  17631  \\nHonestly, it’s been so long since I’ve heard...\n",
       "17632  17632  \\nIt seems like just yesterday that the four N...\n",
       "17633  17633  \\n This is a brilliant album wothy of ten out ...\n",
       "17634  17634  \\n actually worse than his last album!.....has...\n",
       "17635  17635  \\nTheir best album to date. A masterpiece. Ugh...\n",
       "17636  17636  \\nI've waited 15 years for this album. These g...\n",
       "17637  17637  \\n Beautifully dark and brooding. It has great...\n",
       "17638  17638  \\nLemonade é um álbum excelente, com vocais su...\n",
       "17639  17639            \\n A great album from a great artist.\\n\n",
       "17640  17640           \\n bloc party the best band in britain\\n\n",
       "17641  17641  \\nIs This It? is one of the masterpieces of co...\n",
       "17642  17642  \\n Minimalistic like Phillip Glass, but more m...\n",
       "17643  17643  \\n great album, with various elements. Krauss ...\n",
       "17644  17644  \\n Awesome CD - RT singing better than ever - ...\n",
       "17645  17645  \\nI love it, it has been almost a year but it ...\n",
       "17646  17646  \\n Actually, nothing about this album is overr...\n",
       "17647  17647  \\n Wild Mountain Nation may still be their bes...\n",
       "17648  17648  \\nThis is undoubtedly one of the most crisp an...\n",
       "17649  17649  \\n It's a difficult, but absolute amazing reco...\n",
       "17650  17650           \\n Out of this world...nothing like it\\n\n",
       "\n",
       "[17651 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.apply(lambda x: x.astype(str).str.lower())\n",
    "test = test.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# converting pandas to numpy\n",
    "x_train = train[1].values\n",
    "y_train = train[2].values\n",
    "\n",
    "x_test = test[1].values\n",
    "\n",
    "x_test = x_test[1:]\n",
    "x_train = x_train[1:]\n",
    "y_train = y_train[1:]\n",
    "\n",
    "print(len(x_train[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17650,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer,SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z0-9]+\")\n",
    "sw  = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmedReview(review):\n",
    "    \n",
    "   \n",
    "\n",
    "    review = review.replace(\"<br /><br />\",\" \")\n",
    "    \n",
    "    #Tokenize\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "   \n",
    "  \n",
    "    \n",
    "    new_tokens = [token for token in tokens if token not in sw]\n",
    "    new_tokens = [token for token in new_tokens if  token.isalpha()]\n",
    "    new_tokens = [token for token in new_tokens if len(token)>1]\n",
    "   \n",
    "    cleaned_review = ' '.join(new_tokens)\n",
    "\n",
    "    \n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(35833,)\n",
      "(17650,)\n",
      "(35833,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "l = len(x_train)\n",
    "l1 = len(x_test)\n",
    "print('\\n')\n",
    "for i in range(l):\n",
    "    x_train[i] = getStemmedReview(x_train[i])\n",
    "    \n",
    "for i in range(l1):\n",
    "    x_test[i] = getStemmedReview(x_test[i])\n",
    "    \n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "        \n",
    "print(type(y_train))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['things', 'like', 'streamlined', 'accessible', 'dont', 'mean', 'better', 'previous', 'albums', 'anything', 'means', 'disposable', 'doesnt', 'hold', 'severals', 'listens']\n"
     ]
    }
   ],
   "source": [
    "x = pd.DataFrame(x_train)\n",
    "y = pd.DataFrame(y_train)\n",
    "for ix in range(x_train.shape[0]):\n",
    "    review=x[0][ix]\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    x[0][ix]=tokens\n",
    "    \n",
    "print(x[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vince', 'gill', 'amazing', 'artist', 'magnificent', 'tenor', 'voice', 'songwriter', 'musician', 'anyone', 'seen', 'perform', 'live', 'go', 'away', 'thrilled']\n"
     ]
    }
   ],
   "source": [
    "print(x[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gotta admit first heard album thought ok years found returning repeated listens best played feeling mellow quiet introspective moments definitely party album songs much feeling help captured one alltime favorites hear gotta admit first heard album thought ok years found returning repeated listens best played feeling mellow quiet introspective moments definitely party album songs much feeling help captured one alltime favorites hear songs hard edge get live fillmore expand\n"
     ]
    }
   ],
   "source": [
    "xt = pd.DataFrame(x_test)\n",
    "print(xt[0][16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = pd.DataFrame(x_test)\n",
    "\n",
    "for ix in range(x_test.shape[0]):\n",
    "    review=xt[0][ix]\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    xt[0][ix]=tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(xt[0][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35833, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.0', '1.0', '2.0', '3.0', '4.0'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as  np\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_len=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def equal_len(x):\n",
    "    for i in range(len(x[0])):\n",
    "    #split_sen = x_train[i].split()\n",
    "        if (len(x[0][i]) < mean_len):\n",
    "            for j in range(abs(mean_len-len(x[0][i]))):\n",
    "                x[0][i].append(0)\n",
    "        if (len(x[0][i]) > mean_len):\n",
    "            x[0][i] = x[0][i][0:mean_len] \n",
    "    return x\n",
    "#dropout - reduce the length of sentence\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solid', 'piece', 'work', 'features', 'lorde', 'best', 'self', 'melodramatic', 'raw', 'greenlight', 'writer', 'dark', 'personal', 'favorites', 'liabilities', 'loveless', 'trailing', 'behind', 'full', 'listen', 'appreciate', 'lorde', 'efforts', 'able', 'connect']\n"
     ]
    }
   ],
   "source": [
    "print(x[0][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(x[0][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word__vectors = KeyedVectors.load_word2vec_format('C:/Users/Amit/Downloads/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_apple = word__vectors[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(v_apple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ix in range(1,30):\n",
    "#    print(len(x[0][ix]))\n",
    "\n",
    "x = equal_len(x)\n",
    "xt = equal_len(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gostei', 'muito', 'lbum', 'faz', 'muito', 'estilo', 'de', 'sica', 'que', 'beyonc', 'canta', 'tem', 'sicas', 'que', 'se', 'salvam', 'que', 'sorry', 'formation', 'inch', 'love', 'drought', 'essas', 'que', 'citei', 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(xt[0][98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getOutputEmbeddings(X):\n",
    "    \n",
    "    embedding_out = np.zeros((X.shape[0],mean_len,300))\n",
    "    count=0\n",
    "    for ix in range(X.shape[0]):\n",
    "        for jx in range(mean_len):\n",
    "            try:\n",
    "                 embedding_out[ix][jx] = word__vectors[X[0][ix][jx]]\n",
    "                \n",
    "            except:\n",
    "                embedding_out[ix][jx] = np.zeros((300,))\n",
    "            count+=1\n",
    "            \n",
    "    print(count)        \n",
    "    return embedding_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074990\n",
      "529500\n"
     ]
    }
   ],
   "source": [
    "emb_xT = getOutputEmbeddings(x)\n",
    "emb_xt = getOutputEmbeddings(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35833, 30, 300)\n"
     ]
    }
   ],
   "source": [
    "print(emb_xT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 2 ... 3 4 3]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array([int(float(i)) for i in y_train])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y= y_train.astype(int) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 30, 64)            93440     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 126,789\n",
      "Trainable params: 126,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64,input_shape=(mean_len,300),return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64,input_shape=(mean_len,300)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32249 samples, validate on 3584 samples\n",
      "Epoch 1/20\n",
      "32249/32249 [==============================] - 77s 2ms/step - loss: 1.0385 - acc: 0.5587 - val_loss: 0.9288 - val_acc: 0.6018\n",
      "Epoch 2/20\n",
      "32249/32249 [==============================] - 74s 2ms/step - loss: 0.9466 - acc: 0.5967 - val_loss: 0.9356 - val_acc: 0.6122\n",
      "Epoch 3/20\n",
      "32249/32249 [==============================] - 76s 2ms/step - loss: 0.9228 - acc: 0.6033 - val_loss: 0.9050 - val_acc: 0.6180\n",
      "Epoch 4/20\n",
      "32249/32249 [==============================] - 74s 2ms/step - loss: 0.9015 - acc: 0.6143 - val_loss: 0.8982 - val_acc: 0.6222\n",
      "Epoch 5/20\n",
      "32249/32249 [==============================] - 73s 2ms/step - loss: 0.8803 - acc: 0.6228 - val_loss: 0.9091 - val_acc: 0.6256\n",
      "Epoch 6/20\n",
      "32249/32249 [==============================] - 77s 2ms/step - loss: 0.8611 - acc: 0.6321 - val_loss: 0.8870 - val_acc: 0.6278\n",
      "Epoch 7/20\n",
      "32249/32249 [==============================] - 75s 2ms/step - loss: 0.8358 - acc: 0.6403 - val_loss: 0.8959 - val_acc: 0.6275\n",
      "Epoch 8/20\n",
      "32249/32249 [==============================] - 75s 2ms/step - loss: 0.8114 - acc: 0.6521 - val_loss: 0.9133 - val_acc: 0.6211\n",
      "Epoch 9/20\n",
      "32249/32249 [==============================] - 77s 2ms/step - loss: 0.7838 - acc: 0.6679 - val_loss: 0.9535 - val_acc: 0.6094\n",
      "Epoch 10/20\n",
      "32249/32249 [==============================] - 77s 2ms/step - loss: 0.7607 - acc: 0.6782 - val_loss: 0.9257 - val_acc: 0.6208\n",
      "Epoch 11/20\n",
      "32249/32249 [==============================] - 66s 2ms/step - loss: 0.7261 - acc: 0.6920 - val_loss: 0.9637 - val_acc: 0.6010\n",
      "Epoch 12/20\n",
      "32249/32249 [==============================] - 57s 2ms/step - loss: 0.6967 - acc: 0.7063 - val_loss: 1.0520 - val_acc: 0.5661\n",
      "Epoch 13/20\n",
      "32249/32249 [==============================] - 57s 2ms/step - loss: 0.6646 - acc: 0.7230 - val_loss: 1.0256 - val_acc: 0.6102\n",
      "Epoch 14/20\n",
      "32249/32249 [==============================] - 57s 2ms/step - loss: 0.6321 - acc: 0.7373 - val_loss: 1.0469 - val_acc: 0.6063\n",
      "Epoch 15/20\n",
      "32249/32249 [==============================] - 58s 2ms/step - loss: 0.6006 - acc: 0.7536 - val_loss: 1.0350 - val_acc: 0.6069\n",
      "Epoch 16/20\n",
      "32249/32249 [==============================] - 58s 2ms/step - loss: 0.5722 - acc: 0.7662 - val_loss: 1.1547 - val_acc: 0.5809\n",
      "Epoch 17/20\n",
      "32249/32249 [==============================] - 58s 2ms/step - loss: 0.5437 - acc: 0.7818 - val_loss: 1.2004 - val_acc: 0.5882\n",
      "Epoch 18/20\n",
      "32249/32249 [==============================] - 58s 2ms/step - loss: 0.5177 - acc: 0.7921 - val_loss: 1.2079 - val_acc: 0.5848\n",
      "Epoch 19/20\n",
      "32249/32249 [==============================] - 58s 2ms/step - loss: 0.4884 - acc: 0.8076 - val_loss: 1.3006 - val_acc: 0.5868\n",
      "Epoch 20/20\n",
      "32249/32249 [==============================] - 58s 2ms/step - loss: 0.4615 - acc: 0.8211 - val_loss: 1.3606 - val_acc: 0.5871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x235bd86c320>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_acc',patience=3)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.fit(emb_xT,m,batch_size=32,epochs=20,shuffle=True,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['better', 'vulnicura', 'success', 'optimistic', 'traumas', 'typical', 'singleness', 'escape', 'electronic', 'spring', 'femininity', 'telephatic', 'journey', 'hidden', 'place', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(xt[0][91])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(emb_xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 3 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame(pred)\n",
    "print(type(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ix in range(len(pred)):\n",
    "#    if pred[0][ix]==1:\n",
    "#        pred[0][ix]='pos'\n",
    "#    else:\n",
    "#        pred[0][ix]='neg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n",
      "0      4\n",
      "1      4\n",
      "2      3\n",
      "3      3\n",
      "4      4\n",
      "5      2\n",
      "6      2\n",
      "7      3\n",
      "8      4\n",
      "9      4\n",
      "10     4\n",
      "11     3\n",
      "12     4\n",
      "13     4\n",
      "14     4\n",
      "15     4\n",
      "16     4\n",
      "17     3\n",
      "18     2\n",
      "19     3\n",
      "20     3\n",
      "21     4\n",
      "22     3\n",
      "23     4\n",
      "24     4\n",
      "25     3\n",
      "26     4\n",
      "27     4\n",
      "28     3\n",
      "29     4\n",
      "...   ..\n",
      "17620  4\n",
      "17621  4\n",
      "17622  3\n",
      "17623  2\n",
      "17624  4\n",
      "17625  3\n",
      "17626  4\n",
      "17627  4\n",
      "17628  3\n",
      "17629  3\n",
      "17630  3\n",
      "17631  2\n",
      "17632  4\n",
      "17633  1\n",
      "17634  4\n",
      "17635  4\n",
      "17636  4\n",
      "17637  4\n",
      "17638  4\n",
      "17639  4\n",
      "17640  4\n",
      "17641  4\n",
      "17642  4\n",
      "17643  3\n",
      "17644  4\n",
      "17645  2\n",
      "17646  4\n",
      "17647  4\n",
      "17648  4\n",
      "17649  4\n",
      "\n",
      "[17650 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = pred.to_csv (r\"C:/Users/Amit/Documents/output/music_ans9.csv\", header='label') #Don't forget to add '.csv' at the end of the path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
